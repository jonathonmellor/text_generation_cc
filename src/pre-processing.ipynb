{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text exploration and processing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.tokenize import WhitespaceTokenizer\n",
    "from nltk import FreqDist\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import progressbar\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import text data\n",
    "\n",
    "The data is currently one text file, with each line corresponding to one post. The method for extraction from reddit is detailed in the scraper file. \n",
    "\n",
    "The data will first be explored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_posts = 5000\n",
    "file_path = '../raw_data/relationships_{}.txt'.format(num_posts)\n",
    "gram_len = 4\n",
    "\n",
    "with open(file_path, 'r') as file:\n",
    "    raw_relationship_data = file.read()\n",
    "    print(\"file imported\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_relationship_data[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on this first post this raises an interesting point about capital letters. I assumed that we wouldn't need to lowercase all the data, but if this Capitalised Every Word syntax is prevelant then this could be an issue. We will assume that lowercasing will produce a more informative model due to uniformity and lower likelyhood of Out Of Vocab words.\n",
    "\n",
    "We are going to explore the punctuation in the text as a whole to see what may be insignificant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Count of punctuations/special characters\")\n",
    "\n",
    "interesting_punctuation = [\n",
    "    \"\\n\", \".\", \",\", \":\",\";\", \"\\t\", \"?\",\"!\", \"-\", \"(\", \")\", \":(\", \":)\", \"</3\",\n",
    "    \"[\", \"]\", \"'\", '\"', \"<\",\"_\"\n",
    "]\n",
    "\n",
    "for punctuation in interesting_punctuation:\n",
    "    print(repr(punctuation), \"\\t\", raw_relationship_data.count(punctuation))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This shows that we have exactly the right number of `\\n` symbols. The other punctuation may not be relevant as there are not huge numbers of non full stops, question marks and (maybe?) commas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clean data\n",
    "\n",
    "We want the data to take into account certain grammatical and punctuation syntax. Therefore we are going to map certain symbols to another, and to indicate where the end of a sentence is. It must be ensured that there are adequate spaces between relevant tokens or they won't be parse properly. \n",
    "\n",
    "The punctuation that is going to be kept in is:\n",
    "\n",
    "* full stops\n",
    "* question marks\n",
    "* brackets (one type)\n",
    "\n",
    "We are going to convert the text to lower case for all words in order to increase the uniformity of the text.\n",
    "\n",
    "The newline `/n` symbol is going to be converted to ` <END> ` to indicate the end of a post (using the assumtion that posts are one line per post).\n",
    "\n",
    "Should probably be using regular expressions here for better performance but alas this is a first run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lowercase the data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_relationship_data = raw_relationship_data.lower()\n",
    "print(raw_relationship_data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add spaces to the punctuation we want to keep\n",
    "\n",
    "Should replace with a loop to look nicer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_relationship_data = raw_relationship_data.replace(\"<\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\">\", \" \")\n",
    "\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\"\\n\", \" <END> <START> \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\".\", \" . \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"?\", \" ? \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\",\", \" , \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\"[\", \" (\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"]\", \") \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace(\":\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\";\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"-\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"!\", \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"_\", \" \")\n",
    "\n",
    "raw_relationship_data = raw_relationship_data.replace('\"', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"'\", \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace(\"“\", \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('”', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('’', \"\")\n",
    "raw_relationship_data = raw_relationship_data.replace('…', \" \")\n",
    "raw_relationship_data = raw_relationship_data.replace('...', \" , \")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I gave up on not using regular expressions, we can check what non-alpha nums are still within the text.\n",
    "\n",
    "Some of the emojis produced are minorly upsetting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "set(re.sub(r'[A-Za-z0-9 ]', '', raw_relationship_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this we can see there is a wide range of punctuation that is not covered by our replacing procedure. We will remove all:\n",
    "\n",
    "* alphanumerics\n",
    "* full stops, commas, question marks\n",
    "* characters in the `<END>` symbol"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_data = re.sub(r'[^A-Za-z0-9 <>(),.?]', '', raw_relationship_data)\n",
    "relationship_data = re.sub(\"(  \\.  \\.)+\", \" \", relationship_data)\n",
    "relationship_data = re.sub(\"  +\", \" \", relationship_data)\n",
    "\n",
    "print(relationship_data[:200])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set(re.sub(r'[A-Za-z0-9 ]', '', relationship_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The (GENGER_AGE) syntax may be useful to replace with a generic placeholder in order to prevent rare / out of vocab issues, the model will end up predicting some age based on langauge."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not quite sure where to tokenise this data, definitely before creating the sequences but not sure if the data should be sentences first.\n",
    "\n",
    "Will go with before creating sentences.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenization\n",
    "\n",
    "Separate the string into words using spaces to determine a new token. This will make punctuation tokens which is what we want for sentence structure.\n",
    "\n",
    "Could use one of NLTK's casual tokenizer but as we have already preprocessed the strings for our own purpose the standard one may do fine. EDIT: as we have processed out words and punctuation to have whitespace where appropriate the WhitespaceTokenizer is best here.\n",
    "EDIT2: We don't use these tokenised words later, but they're interesting to look at / explore the WS tokeniser."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws_tk = WhitespaceTokenizer() \n",
    "\n",
    "relationships_word_tokened = ws_tk.tokenize(relationship_data)\n",
    "\n",
    "print(relationships_word_tokened[:50])\n",
    "\n",
    "print(\"Number of tokens total:\", len(relationships_word_tokened))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly many of our most common words are stop words, but these are important to our sentence structure so they will be kept in. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We may choose the use the sentence structure of our data instead of a bag of words model, this will mean tokenising the sentences as well as words. I've done this kind of backwards as the `\\n` strings denoted new posts previously but now we get a string for each post that has been cleaned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_data_sents = relationship_data.split(\" <END> <START> \")\n",
    "relationship_data_sents[0] = relationship_data_sents[0].replace(\"<START>\", \"\")\n",
    "\n",
    "\n",
    "print(relationship_data_sents[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get some more information about our posts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relationship_data_sents_words = (ws_tk.tokenize(post) for post in relationship_data_sents if post)\n",
    "\n",
    "MAX_SEQ_LENGTH = len(max(relationship_data_sents_words, key=len))\n",
    "\n",
    "relationship_data_sents_words = (ws_tk.tokenize(post) for post in relationship_data_sents if post)\n",
    "\n",
    "MIN_SEQ_LENGTH = len(min(relationship_data_sents_words, key=len))\n",
    "\n",
    "\n",
    "print(\"Max post length: \", MAX_SEQ_LENGTH, \"\\n\\n\")\n",
    "print(\"Min post length: \", MIN_SEQ_LENGTH, \"\\n\\n\")\n",
    "\n",
    "relationship_data_sents_words = [ws_tk.tokenize(post) for post in relationship_data_sents if post]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have a list containing each post, within each post is a list of each token within the post. The longest post is given by `MAX_SEQ_LENGTH`\n",
    "\n",
    "### Generate vocab\n",
    "***Warning***\n",
    "\n",
    "The below step will take a time in the order of minutes if # posts >10,000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# this has to be done after tokenisation or it will count strings\n",
    "vocab = set(relationships_word_tokened)\n",
    "len_vocab = len(vocab) + 1\n",
    "print(\"Vocab length: \", len_vocab)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to convert the word data into integers the model will be able to understand, a little bit cheating but keras has a nice way to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "# convert the posts to embeddings\n",
    "keras_embedder = Tokenizer(num_words=None, filters=[], lower=False, split=\" \")\n",
    "\n",
    "keras_embedder.fit_on_texts(relationship_data_sents)\n",
    "\n",
    "embedded_sents = keras_embedder.texts_to_sequences(relationship_data_sents)\n",
    "\n",
    "print(len(embedded_sents))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a function so that we can keep only sequences with the most frequent words (by numbeer of occurances in the corpus. This should allow the model to generalise better.)\n",
    "\n",
    "Shouldn't convert from words -> embeddings -> words but oh well.\n",
    "\n",
    "Total overall count in corpus possibly not the best metric to determine inclusion, tf-idf? But simple to impliment and should get the right result of reducing noise.\n",
    "\n",
    "Possibly also drop very unique trigrams? Would reduce serendipity, but make easier predictions\n",
    "\n",
    "Some embedding values will have no data but the model should be able to handle that *gestures wildly*.\n",
    "\n",
    "The below function is implimented to speed up trainng and imrpove predictions by keeping in only sequences which contain words that are used above some (arbitrary?) frequency. The model will not likely ever predict a word that has only been used once in it's corpus. This will alter the distribution of words and impact overfitting, but it seems sensible to introduce the ability to control for frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "TOKEN_FREQ_DICT = Counter(relationships_word_tokened)\n",
    "\n",
    "# we have an embedder to go from words to numbers, this dict goes numbers to words\n",
    "# there is probably an implimentation w/in keras for this but..\n",
    "\n",
    "INV_EMBEDDER = {index: word for word, index in keras_embedder.word_index.items()}\n",
    "\n",
    "def check_freq(sequence, threshold):\n",
    "    \"\"\"Returns True/False if sequence contains token that is frequen enough\"\"\"\n",
    "    keep = False\n",
    "    for index in sequence:\n",
    "        word = INV_EMBEDDER[index]\n",
    "        if TOKEN_FREQ_DICT[word] > threshold:\n",
    "            keep = True\n",
    "    return keep\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick exploration of the most frequent words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "most_common_40 = TOKEN_FREQ_DICT.most_common()[:40]\n",
    "\n",
    "words, counts = list(zip(*most_common_40))\n",
    "\n",
    "plt.bar(words, list(counts))\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Appears to be (unsurprisingly) mostly stop words + topic related nouns (boyfriend, girlfriend, friend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import functools\n",
    "import operator\n",
    "import itertools\n",
    "\n",
    "keep_in_if_more_than = 5\n",
    "\n",
    "sequences = []\n",
    "\n",
    "#consider converting `sequences` to a generator format\n",
    "embedded_sents_right_len = (post for post in embedded_sents if len(post) > gram_len)\n",
    "for post_num, post in enumerate(embedded_sents_right_len):\n",
    "    if post_num % 1000 == 0:\n",
    "        print(\"Post number\", post_num)\n",
    "    for index in range(gram_len-1, len(post)):\n",
    "        single_sequence = post[index-gram_len-1:index+1]\n",
    "        if check_freq(single_sequence, keep_in_if_more_than):\n",
    "            # other methods for appending tested for speed\n",
    "            #sequences = sequence + singe_sequence\n",
    "            #sequences = itertools.chain(single_sequence, sequences)\n",
    "            sequences.append(single_sequence)\n",
    "            \n",
    "print(\"loop end\")\n",
    "\n",
    "\n",
    "flattened_sequences = functools.reduce(operator.concat, sequences)\n",
    "print(\"Total words: {}\".format(len(flattened_sequences)))\n",
    "print(\"Number unique words: {}\".format(len(set(flattened_sequences))))\n",
    "print(\"Number of sequences: {}\".format(len(sequences)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an embedding for each post. We can now make the train/\n",
    "predict seqence pairs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to ensure all our sequences are padded adequately, they should already be now by prunning non-`gram_len` lengths, but it doesn't (if you exclude time) hurt to add this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "padded_sequences = pad_sequences(np.array(sequences), maxlen=gram_len, padding='pre')\n",
    "        \n",
    "print(padded_sequences.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert the sequences to features + targets in order to train a model in a categorical manner.\n",
    "\n",
    "Would be good to see if this is easier in a vectorised or functional approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "\n",
    "# split into input and output elements\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "#some wrongly formatted sequences were passing through\n",
    "padded_sequences = (np.array(each) for each in padded_sequences \n",
    "                    if each != [] and len(each) > 1)\n",
    "\n",
    "# y is the final element of each sequence\n",
    "for each_seq in padded_sequences:\n",
    "    X_each_seq, y_each_seq = each_seq[:-1], each_seq[-1]\n",
    "    X.append(X_each_seq)\n",
    "    y.append(y_each_seq)\n",
    "\n",
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "X, y = shuffle(X, y, random_state=0)\n",
    "\n",
    "print(X.shape)\n",
    "print(y.shape)\n",
    "print(y[:20])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Save the data to be accessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import time\n",
    "\n",
    "timestr = time.strftime(\"%Y%m%d-%H%M%S\")\n",
    "\n",
    "pickle_file = \"../processed_data/{}-{}-{}-{}\".format(timestr, keep_in_if_more_than, gram_len,\n",
    "                                                    num_posts)\n",
    "\"\"\"\n",
    "with open(pickle_file, \"wb\") as f:\n",
    "    pickle.dump((X, y, keras_embedder, len_vocab, gram_len), f)\n",
    "\n",
    "   \"\"\" \n",
    " \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Open file just written to check contents\n",
    "with open(pickle_file, \"rb\") as f:\n",
    "    X, y, keras_embedder, len_vocab, gram_len = pickle.load(f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
